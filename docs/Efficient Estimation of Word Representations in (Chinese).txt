https://arxiv.org/pdf/1301.3781
Efficient Estimation of Word Representations in Vector Space

1 引言
许多当前的自然语言处理（NLP）系统和技术将词语视为原子单元——词与词之间没有相似性概念，因为它们在词汇表中被表示为索引。这个选择有几个很好的理由——简单性、稳健性，以及简单模型在大量数据上训练优于在较少数据上训练的复杂系统的观察。例如，流行的N元语法模型被用于统计语言建模——如今，几乎可以对所有可用数据（数万亿个词[3]）进行N元语法的训练。

然而，简单的技术在许多任务中已经达到极限。例如，自动语音识别领域的相关数据量是有限的——其性能通常受高质量转录语音数据量的限制（通常只有数百万词）。在机器翻译中，许多语言的现有语料库仅包含几十亿个词甚至更少。因此，在某些情况下，单纯扩大基本技术的规模并不会带来显著的进展，我们必须专注于更高级的技术。

随着近年来机器学习技术的进步，在更大规模数据集上训练更复杂的模型变得可能，而这些模型通常优于简单模型。或许最成功的概念是使用词的分布式表示[10]。例如，基于神经网络的语言模型显著优于N元语法模型[1, 27, 17]。

1.1 本文目标
本文的主要目标是介绍用于从包含数十亿词语的大型数据集以及包含数百万词汇的词汇表中学习高质量词向量的技术。据我们所知，之前提出的架构中，没有任何一种成功地在超过几亿个词语的数据上进行训练，词向量的维度仅在50-100之间。

我们采用了最近提出的技术来衡量生成的向量表示的质量，期望不仅相似的词倾向于彼此靠近，而且词语可以表现出多重相似度层次[20]。在屈折语的研究中早已观察到这一点——例如，名词可以有多种词尾变化，如果我们在原始向量空间的子空间中搜索相似词，可能会找到具有相似词尾的词语[13, 14]。

有些令人惊讶的是，词表示的相似性不仅限于简单的句法规则。通过词偏移技术（即对词向量进行简单的代数操作），例如“King”的向量减去“Man”的向量再加上“Woman”的向量，结果是最接近“Queen”一词的向量表示[20]。

在本文中，我们试图通过开发新的模型架构来保持词语之间的线性规律，从而最大化这些向量操作的准确性。我们设计了一个新的综合测试集，用于测量句法和语义规律性<1>，并展示了许多此类规律可以被高精度地学习。此外，我们还讨论了训练时间和准确性如何取决于词向量的维度和训练数据的量。

1.2 相关工作
将词表示为连续向量的历史悠久[10, 26, 8]。一种非常流行的神经网络语言模型（NNLM）估计架构是在[1]中提出的，该架构使用带有线性投影层和非线性隐藏层的前馈神经网络，同时学习词向量表示和统计语言模型。这项工作得到了许多后续研究的支持。

另一种有趣的NNLM架构在[13, 14]中被提出，首先使用单隐藏层的神经网络学习词向量，然后使用这些词向量训练NNLM。因此，即使不构建完整的NNLM，词向量也能被学习。在本研究中，我们直接扩展了这一架构，专注于第一个步骤，即使用简单模型学习词向量。

后来证明，词向量可以显著改进和简化许多自然语言处理（NLP）应用[4, 5, 29]。词向量的估计是通过不同的模型架构并在各种语料库上进行训练的[4, 29, 23, 19, 9]，一些生成的词向量也被用于未来的研究和比较<2>。然而，据我们所知，这些架构的训练成本比[13]中提出的架构要高得多，除了一些使用对角权重矩阵的对数双线性模型版本[23]。

2 模型架构
许多不同类型的模型被提出用于估计词语的连续表示，包括著名的潜在语义分析（LSA）和潜在狄利克雷分配（LDA）。在本文中，我们重点关注通过神经网络学习到的词的分布式表示，因为已有研究表明，它们在保持词语之间的线性规律性方面表现得比LSA显著更好[20, 31]；此外，LDA在大数据集上计算开销极高。

类似于[18]的做法，为了比较不同的模型架构，我们首先定义模型的计算复杂度为完全训练模型所需访问的参数数量。接下来，我们将尝试在最小化计算复杂度的同时，最大化准确性。

<1> The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt
<2> http://ronan.collobert.com/senna/
http://metaoptimize.com/projects/wordreprs/
http://www.fit.vutbr.cz/˜imikolov/rnnlm/
http://ai.stanford.edu/˜ehhuang/


对于以下所有模型，训练复杂度与以下公式成正比： 

	O = E x T x Q,	(1)

其中，E 是训练周期数，T 是训练集中的词语数量，而 Q 将在每个模型架构中进一步定义。常见的选择是 E=3−50，而 T 可以高达十亿。所有模型都使用随机梯度下降和反向传播进行训练[26]。

2.1 前馈神经网络语言模型 (NNLM)

概率前馈神经网络语言模型最早在[1]中提出。它由输入层、投影层、隐藏层和输出层组成。在输入层，前 N 个词通过 1-of-V 编码进行编码，其中 V 是词汇表的大小。输入层然后被投影到一个维度为 N x D 的投影层 P，使用共享的投影矩阵。由于任意时刻只有 N 个输入处于活动状态，投影层的组合是一种相对较廉价的操作。

然而，在投影层和隐藏层之间的计算变得复杂，因为投影层中的值是稠密的。对于常见的选择 N=10，投影层 P 的大小可能为 500 到 2000，而隐藏层的大小 H 通常为 500 到 1000 单元。此外，隐藏层用于计算词汇表中所有词的概率分布，导致输出层的维度为 V。因此，每个训练样本的计算复杂度为：

	Q = N x D + N x D x H + H x V,

其中占主导地位的项是 H x V。然而，几种实用的解决方案被提出以避免这一问题；可以使用分层的 softmax 版本[25, 23, 18]，或者通过在训练时使用非归一化的模型完全避免归一化模型[4, 9]。使用词汇表的二叉树表示时，需要评估的输出单元数量可以减少到大约 log⁡2(V)。因此，大多数复杂性来自于 N x D x H 项。

在我们的模型中，我们使用了分层 softmax，其中词汇表被表示为一个 Huffman 二叉树。这是基于之前的观察，即词频在神经网络语言模型中非常适合用于获取类[16]。Huffman 树为高频词分配较短的二进制编码，这进一步减少了需要评估的输出单元数量：尽管平衡二叉树需要评估 log2​(V) 个输出，但基于 Huffman 树的分层 softmax 只需要评估大约 log⁡2(Unigram perplexity(V))。例如，当词汇表大小为一百万词时，这大约可以使评估速度提升两倍。尽管这对于神经网络语言模型来说不是关键的速度提升，因为计算瓶颈在于 N x D x H 项，但我们将在后面提出不含隐藏层的架构，因此这类架构将严重依赖于 softmax 归一化的效率。








